{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b350127ce8f346b8bef9b5f29db10341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e5418796d0b43938a83b55f705479e0",
              "IPY_MODEL_21feb7499e2e47eb83e5627948487e24",
              "IPY_MODEL_941328f30f134e2fabe213be1a51fa85"
            ],
            "layout": "IPY_MODEL_249556e809ac42f3adde0e987d6114f6"
          }
        },
        "0e5418796d0b43938a83b55f705479e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3de9e2cc1d414eb3a3ce392799390e85",
            "placeholder": "​",
            "style": "IPY_MODEL_63fc03d11ffd4a1baec6b763141b1031",
            "value": "100%"
          }
        },
        "21feb7499e2e47eb83e5627948487e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b561c51537542ae8017ea31b9944e31",
            "max": 354476359,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2d749886ee4480b85417ead83144280",
            "value": 354476359
          }
        },
        "941328f30f134e2fabe213be1a51fa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b87176c8f86d4c35b0cc8be9a3e0dd5a",
            "placeholder": "​",
            "style": "IPY_MODEL_70ebacdd135d437998fc28bd9163920f",
            "value": " 338M/338M [00:08&lt;00:00, 66.8MB/s]"
          }
        },
        "249556e809ac42f3adde0e987d6114f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3de9e2cc1d414eb3a3ce392799390e85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63fc03d11ffd4a1baec6b763141b1031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b561c51537542ae8017ea31b9944e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d749886ee4480b85417ead83144280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b87176c8f86d4c35b0cc8be9a3e0dd5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ebacdd135d437998fc28bd9163920f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7-5wtxuvF1-",
        "outputId": "891dfb83-33bb-40ce-d241-d9628dcf8380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IymsYwQOy-__",
        "outputId": "6842dbd6-ac5a-4003-b377-2e678ee4a348"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.1.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 36 kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQoxGhKt0Frv",
        "outputId": "246ca99b-6d9a-45ac-8871-ea9087fc8f60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from typing import Iterable\n",
        "\n",
        "def clip_and_normalize(np_image: np.ndarray,\n",
        "                       clip_min: int = -1100,\n",
        "                       clip_max: int = 300\n",
        "                       ) -> np.ndarray:\n",
        "    np_image = np.clip(np_image, clip_min, clip_max)\n",
        "    np_image = (np_image - clip_min) / (clip_max - clip_min)\n",
        "    return np_image\n",
        "\n",
        "def resample(itk_image: sitk.Image,\n",
        "             new_spacing: Iterable[float],\n",
        "             outside_val: float = 0\n",
        "             ) -> sitk.Image:\n",
        "\n",
        "    shape = itk_image.GetSize()\n",
        "    spacing = itk_image.GetSpacing()\n",
        "    output_shape = tuple(int(round(s * os / ns)) for s, os, ns in zip(shape, spacing, new_spacing))\n",
        "    return sitk.Resample(\n",
        "        itk_image,\n",
        "        output_shape,\n",
        "        sitk.Transform(),\n",
        "        sitk.sitkLinear,\n",
        "        itk_image.GetOrigin(),\n",
        "        new_spacing,\n",
        "        itk_image.GetDirection(),\n",
        "        outside_val,\n",
        "        sitk.sitkFloat32,\n",
        "    )\n",
        "\n",
        "def center_crop(np_image: np.ndarray,\n",
        "                new_shape: Iterable[int],\n",
        "                outside_val: float = 0\n",
        "                ) -> np.ndarray:\n",
        "    output_image = np.full(new_shape, outside_val, np_image.dtype)\n",
        "\n",
        "    slices = tuple()\n",
        "    offsets = tuple()\n",
        "    for it, sh in enumerate(new_shape):\n",
        "        size = sh // 2\n",
        "        if it == 0:\n",
        "            center = np_image.shape[it] - size\n",
        "        else:\n",
        "            center = (np_image.shape[it] // 2)\n",
        "        start = center - size\n",
        "        stop = center + size + (sh % 2)\n",
        "\n",
        "        slce = slice(max(0, start), min(np_image.shape[it], stop))\n",
        "        slices += (slce,)\n",
        "\n",
        "        offset = slice(-min(0, start), 2 * size - max(0, (start + 2 * size) - np_image.shape[it]))\n",
        "        offsets += (offset,)\n",
        "\n",
        "    output_image[offsets] = np_image[slices]\n",
        "\n",
        "    return output_image\n",
        "\n",
        "\n",
        "class stoic2021(Dataset) :\n",
        "    def __init__(self,root,csvpath,slices):\n",
        "        self.root=root\n",
        "        self.csvpath=csvpath\n",
        "        self.pathd=self.csvpath\n",
        "        self.patient_id=self.pathd['PatientID']\n",
        "        self.slices=slices\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        patient_id=self.pathd['PatientID'][index]\n",
        "        labels=np.array(self.pathd[['probCOVID','probSevere']],dtype=float)[index]\n",
        "        patient_path=os.path.join(self.root,str(patient_id)+'.mha')\n",
        "        input_image = sitk.ReadImage(patient_path)\n",
        "        process_vol=self._preprocess(input_image)\n",
        "        \n",
        "        process_vol=self._slice_extraction(process_vol)\n",
        "        torch_img=torch.from_numpy(process_vol)\n",
        "        torch_img=torch.unsqueeze(torch_img,axis=0)\n",
        "        torch_img=torch.unsqueeze(torch_img,axis=2)\n",
        "        process_vol=torch_img\n",
        "        return process_vol,labels\n",
        "        \n",
        "    \n",
        "    def _preprocess(self,input_image: sitk.Image,\n",
        "               new_spacing: Iterable[float] = (1.6, 1.6, 1.6),\n",
        "               new_shape: Iterable[int] = (240, 240, 240),\n",
        "               ) -> np.ndarray:\n",
        "        input_image = resample(input_image, new_spacing=new_spacing)\n",
        "        input_image = sitk.GetArrayFromImage(input_image)\n",
        "        input_image = center_crop(input_image, new_shape=new_shape)\n",
        "        input_image = clip_and_normalize(input_image)\n",
        "\n",
        "        return input_image\n",
        "    def _slice_extraction(self,im):\n",
        "      lis=[]\n",
        "      z=9\n",
        "      for i in range(1,self.slices):\n",
        "          if i==0:\n",
        "            val=im[:,:,i]\n",
        "          else:\n",
        "            val=im[:,:,z]\n",
        "            z=z+10\n",
        "          lis.append(val)\n",
        "      vol=np.array(lis)\n",
        "      return vol\n",
        "    \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.patient_id)"
      ],
      "metadata": {
        "id": "2_V7r39Tvqbm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/Dataset/data/mha'\n",
        "\n",
        "csvpath='/content/drive/MyDrive/Dataset/metadata/reference.csv'\n",
        "data=pd.read_csv(csvpath)\n",
        "valid_frac = 0.3\n",
        "\n",
        "valid_df = data.sample(frac = valid_frac)\n",
        "train_df = data.drop(valid_df.index)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "dataset_train = stoic2021(path, train_df, slices=25)\n",
        "dataset_valid = stoic2021(path, valid_df, slices=25)"
      ],
      "metadata": {
        "id": "YYHKefAQ0lF6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "class Block(nn.Module):\n",
        "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "    \n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) \n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim) \n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
        "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) \n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2) \n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    def __init__(self, in_chans=3, num_classes=1000, \n",
        "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
        "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList() \n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList() \n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
        "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1])) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError \n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "  "
      ],
      "metadata": {
        "id": "3oyGlzIP0sQt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "url = \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\"\n",
        "model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024])\n",
        "checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "model.load_state_dict(checkpoint[\"model\"])\n",
        "\n",
        "class CFG:\n",
        "    img_size = 240\n",
        "    n_frames = 24\n",
        "    \n",
        "    cnn_features = 240\n",
        "    lstm_hidden = 32\n",
        "    \n",
        "    n_fold = 5\n",
        "    n_epochs = 15\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.map = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1)\n",
        "        self.net =model\n",
        "        self.net.head = nn.Linear(in_features=1024, out_features=240, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.map(x))\n",
        "        out = self.net(x)\n",
        "        return out\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cnn = CNN()\n",
        "        self.rnn = nn.LSTM(CFG.cnn_features, CFG.lstm_hidden, 2, batch_first=True)\n",
        "        self.fc = nn.Linear(CFG.lstm_hidden, 2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: BxTxCxHxW\n",
        "        #print(x.size())\n",
        "        batch_size, _, timesteps, C, H, W = x.size()\n",
        "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
        "        #print(c_in.shape)\n",
        "        c_out = self.cnn(c_in)\n",
        "        r_in = c_out.view(batch_size, timesteps, -1)\n",
        "        #print(r_in.shape)\n",
        "        output, (hn, cn) = self.rnn(r_in)\n",
        "        \n",
        "        out = self.fc(hn[-1])\n",
        "        return out\n",
        "model = Model()\n",
        "model=nn.DataParallel(model)\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b350127ce8f346b8bef9b5f29db10341",
            "0e5418796d0b43938a83b55f705479e0",
            "21feb7499e2e47eb83e5627948487e24",
            "941328f30f134e2fabe213be1a51fa85",
            "249556e809ac42f3adde0e987d6114f6",
            "3de9e2cc1d414eb3a3ce392799390e85",
            "63fc03d11ffd4a1baec6b763141b1031",
            "6b561c51537542ae8017ea31b9944e31",
            "f2d749886ee4480b85417ead83144280",
            "b87176c8f86d4c35b0cc8be9a3e0dd5a",
            "70ebacdd135d437998fc28bd9163920f"
          ]
        },
        "id": "ANwTWoen0v6L",
        "outputId": "16d17c01-9db8-4354-cd3a-5599486118ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/338M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b350127ce8f346b8bef9b5f29db10341"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=6,pin_memory=True,num_workers=2,shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset_valid, batch_size=6,pin_memory=True,num_workers=2,shuffle=False)"
      ],
      "metadata": {
        "id": "RFYg25Ty00Q9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "#from sklearn.metrics import roc_auc_score\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
        "model=nn.DataParallel(model)\n",
        "model.to(device)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.5, last_epoch=-1, verbose=True)\n",
        "from collections import OrderedDict\n",
        "def train(train_dataloader, model, criterion, optimizer, epoch, scheduler):\n",
        "    losses_avg, auc_avg = [], []\n",
        "    train_loss, auc_train = [], []\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for i,data in enumerate(tqdm(train_dataloader)):\n",
        "        # extract dataset\n",
        "        imge,label=data\n",
        "        imge=imge.float()\n",
        "        label=label.float()\n",
        "        imge=imge.to(device)\n",
        "        label=label.to(device)\n",
        "        # zero_out the gradient\n",
        "        optimizer.zero_grad()\n",
        "        #print(imge.size())\n",
        "        output=model(imge)\n",
        "        loss=criterion(output,label)\n",
        "        train_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        output=torch.sigmoid(output)\n",
        "        pred=output.detach().cpu().numpy().astype(float)\n",
        "        y=label.detach().cpu().numpy()\n",
        "        pred1 = np.array(pred > 0.5, dtype=float)\n",
        "        try:\n",
        "            auc=roc_auc_score(y.flatten(), pred1.flatten())\n",
        "        except ValueError:\n",
        "            pass\n",
        "        auc_train.append(auc)\n",
        "    scheduler.step()  \n",
        "    losses_avg=np.mean(train_loss)\n",
        "    auc_avg=np.mean(auc_train)\n",
        "    \n",
        "    log = OrderedDict([('loss', losses_avg),('auc', auc_avg),])\n",
        "    return log\n",
        "\n",
        "def validate(valid_dataloader, model, criterion):\n",
        "    test_loss, auc_val = [], []\n",
        "    losses_avg, auc_avg = [], []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i,data in enumerate(tqdm(valid_dataloader)):\n",
        "            imge,label=data\n",
        "            imge=imge.float()\n",
        "            label=label.float()\n",
        "            imge=imge.to(device)\n",
        "            label=label.to(device)\n",
        "            output=model(imge)\n",
        "            loss=criterion(output,label)\n",
        "            test_loss.append(loss.item())\n",
        "            output=torch.sigmoid(output)\n",
        "            pred=output.detach().cpu().numpy().astype(float)\n",
        "            y=label.detach().cpu().numpy()\n",
        "            pred1 = np.array(pred > 0.5, dtype=float)\n",
        "            try:\n",
        "                auc_va=roc_auc_score(y.flatten(), pred1.flatten())\n",
        "            except ValueError:\n",
        "                pass\n",
        "            auc_val.append(auc_va)\n",
        "    losses_avg=np.mean(test_loss)\n",
        "    auc_avg=np.mean(auc_val)\n",
        "    log = OrderedDict([('loss', losses_avg),('auc', auc_avg),])\n",
        "    \n",
        "    return log\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "log = pd.DataFrame(index=[], columns=['epoch', 'lr', 'loss', 'auc', 'val_loss', 'val_auc'])\n",
        "early_stop=20\n",
        "epochs=100\n",
        "best_auc = 0\n",
        "lr=0.01\n",
        "name='3DDensUnetf3'\n",
        "trigger = 0\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch [%d/%d]' %(epoch, epochs))\n",
        "    # train for one epoch\n",
        "    train_log = train(train_dataloader, model, criterion, optimizer, epoch,scheduler)\n",
        "    # evaluate on validation set\n",
        "    val_log =validate(valid_dataloader, model, criterion)\n",
        "    print('loss %.4f - auc %.4f - val_loss %.4f - val_auc %.4f'%(train_log['loss'], train_log['auc'], \n",
        "                                                                 val_log['loss'], val_log['auc']))\n",
        "\n",
        "    tmp = pd.Series([epoch,lr,train_log['loss'],train_log['auc'],val_log['loss'],val_log['auc']], \n",
        "                    index=['epoch', 'lr', 'loss', 'auc', 'val_loss', 'val_auc'])\n",
        "\n",
        "    log = log.append(tmp, ignore_index=True)\n",
        "    log.to_csv('models/%s/log_3dcovid.csv' %name, index=False)\n",
        "\n",
        "    trigger += 1\n",
        "\n",
        "    if val_log['auc'] > best_auc:\n",
        "        torch.save(model.state_dict(), 'models/%s/model_densnet3d.pth' %name)\n",
        "        best_auc = val_log['auc']\n",
        "        print(\"=> saved best model\")\n",
        "        trigger = 0\n",
        "\n",
        "    # early stopping\n",
        "    if not early_stop is None:\n",
        "        if trigger >= early_stop:\n",
        "            print(\"=> early stopping\")\n",
        "            break\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"done training\")"
      ],
      "metadata": {
        "id": "c7juGfSS09PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VHXrom-b5BWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}